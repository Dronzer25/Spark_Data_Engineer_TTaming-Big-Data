{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace5dd3-9ea9-4b23-859e-bdbd7f9b8605",
   "metadata": {},
   "source": [
    "# Sample Apache Spark Notebook\n",
    "\n",
    "Here's an example of cleaning and analyzing an Apache access log, but this time within an interactive Notebook environment!\n",
    "\n",
    "While prototyping data engineering solutions, notebook environments are popular.\n",
    "\n",
    "If you installed the pyspark package or are working within an already-established environment for Spark, things will probably \"just work.\" But if not, using the findspark package will tie the notebook to your existing Spark installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cffd814-9ac3-436f-8302-828e812e04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\91932\\anaconda3\\envs\\py310\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910ebf75-0a78-4b7f-9abc-d8fdd0093093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e663604-b27d-4b02-bade-5e11f46a0a4a",
   "metadata": {},
   "source": [
    "One nice thing about notebooks is that you can leave little comments and explanations like this, in markdown format.\n",
    "\n",
    "We'll start by importing the stuff we need, and creating a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "944a1921-c630-4280-90b0-5d662480bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, count, desc\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ApacheLogAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bfe8e-97cc-4520-b2ed-8066bfeab7e4",
   "metadata": {},
   "source": [
    "Next we'll load up our sample access log, and load in the raw text into a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f70e75-94d2-4fc4-9362-dc5d9cd6135b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/SparkCourse/ml-100k/access_log.txt. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m log_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ml-100k/access_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read log file as text\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m logs_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py:713\u001b[0m, in \u001b[0;36mDataFrameReader.text\u001b[1;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[0;32m    711\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [paths]\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/SparkCourse/ml-100k/access_log.txt. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "# Define log file path (Update this path to your log file location)\n",
    "log_file = \"./ml-100k/access_log.txt\"\n",
    "\n",
    "# Read log file as text\n",
    "logs_df = spark.read.text(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fefad-fb71-49eb-8eda-a2482d7579b3",
   "metadata": {},
   "source": [
    "We'll now parse the log into the fields we are interested in. Note, we know there is some bad data in here where the status code is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece337f-2f61-49b1-8b72-56cdec462fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern to extract fields\n",
    "log_pattern = r'(\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\"'\n",
    "\n",
    "# Extract fields using regex\n",
    "parsed_logs_df = logs_df.select(\n",
    "    regexp_extract('value', log_pattern, 1).alias(\"ip_address\"),\n",
    "    regexp_extract('value', log_pattern, 2).alias(\"timestamp\"),\n",
    "    regexp_extract('value', log_pattern, 3).alias(\"request\"),\n",
    "    regexp_extract('value', log_pattern, 4).alias(\"status\"),\n",
    "    regexp_extract('value', log_pattern, 5).cast(\"integer\").alias(\"bytes\"),\n",
    "    regexp_extract('value', log_pattern, 6).alias(\"user_agent\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6194c-6691-41c4-987b-19ec9d2c792f",
   "metadata": {},
   "source": [
    "We'll use a filter to just remove those bogus rows with no status, and cast the remaining status codes to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c34fb-fc1b-47e1-8ef9-d699f2664ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with empty status fields\n",
    "cleaned_df = parsed_logs_df.filter(col(\"status\").isNotNull() & (col(\"status\") != \"\"))\n",
    "cleaned_df = cleaned_df.withColumn(\"status\", col(\"status\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b06dcd-a1d8-4af9-800f-672cd50c41f3",
   "metadata": {},
   "source": [
    "A nice thing about notebooks is you can break up your processing into these separate blocks, and inspect the output just for whatever it is you're doing. Then you can go back and iterate on that piece of code as needed, rather than re-running everything.\n",
    "\n",
    "Let's further process our data to split out the method and endpoint from the request field, and then preview the resulting Dataframe we have thusfar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b860a8f-977b-4018-b36c-a9378ad93acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split request field to get HTTP method and endpoint\n",
    "parsed_logs_df = cleaned_df.withColumn(\"method\", regexp_extract(\"request\", r'(\\S+)', 1)) \\\n",
    "                               .withColumn(\"endpoint\", regexp_extract(\"request\", r' (\\S+) ', 1))\n",
    "\n",
    "# Show parsed log data\n",
    "parsed_logs_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1b2c-bb61-4e38-ad39-aba7c619da18",
   "metadata": {},
   "source": [
    "Now let's start doing some analysis. We'll start with displaying the top 10 IP addresses. Unfortunately, as usual, there's a hacker trying to DOS me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ced343-cde5-4c6c-a835-f08a6fc7cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count requests per IP address\n",
    "ip_count_df = parsed_logs_df.groupBy(\"ip_address\").agg(count(\"*\").alias(\"request_count\")).orderBy(desc(\"request_count\"))\n",
    "ip_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b7b59-49e9-43c4-98a2-5972e3a9d442",
   "metadata": {},
   "source": [
    "Let's look at the top endpoints, and right away we can see that our friend is trying to break into my WordPress site through xmlrpc.php vulnerabilities and trying to brute-force their way in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592ffaf-cec4-4101-babe-4f631f12b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Most requested endpoints\n",
    "endpoint_count_df = parsed_logs_df.groupBy(\"endpoint\").agg(count(\"*\").alias(\"endpoint_count\")).orderBy(desc(\"endpoint_count\"))\n",
    "endpoint_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3703ce2-66df-4237-a0f8-89fc55a00e05",
   "metadata": {},
   "source": [
    "Let's also take a look at the top status codes. Looks like they've succeeded in making my site unstable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849fd5e-a529-4688-9201-d93300e4e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HTTP status code distribution\n",
    "status_count_df = parsed_logs_df.groupBy(\"status\").agg(count(\"*\").alias(\"status_count\")).orderBy(desc(\"status_count\"))\n",
    "status_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1fcdd-dd63-477f-96c3-3512a4baa67b",
   "metadata": {},
   "source": [
    "Finally we'll shut things down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa05da8-3c09-49fd-91fe-824e647dd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310b049-80f6-47b2-950a-d5b60dad2c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
